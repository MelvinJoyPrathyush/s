import os
import pandas as pd
import logging
import time
import json
from datetime import datetime
from delta.exceptions import ConcurrentAppendException
from pyspark.sql import SparkSession
from pyspark.sql.functions import input_file_name, col, current_timestamp, lit
from pyspark.sql.types import StringType, TimestampType, DoubleType, StructType, StructField
from delta.tables import DeltaTable
from pyspark.sql.functions import sha2, concat_ws
import time
from delta.exceptions import ConcurrentAppendException
from pyspark.sql.utils import AnalysisException
from concurrent.futures import ThreadPoolExecutor, as_completed

# Set up logging to console
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Suppress specific warnings from Spark
spark_logger = logging.getLogger('py4j')
spark_logger.setLevel(logging.ERROR)

# Add a sample log message to verify logging
logger.info("Starting the ingestion process...")


# Initialize Spark session
spark = SparkSession.builder.appName("V3 Streaming Ingestion").enableHiveSupport().getOrCreate()
sc = spark.sparkContext

# Enable schema auto-merge
spark.conf.set("spark.databricks.delta.schema.autoMerge.enabled", "true")

# Define source and target paths
source_base_path = "s3://aws-gb-dataenhacement-uat-np/datalake_manuuat"
target_base_path = "abfss://gb-dl-raw-v3-pii@cdngbcacnpdevdl.dfs.core.windows.net/ingestion_Jun2"
config_file_path = "abfss://gb-ingestion-v3-config@cdngbcacnpdevplconfdl.dfs.core.windows.net/config/First_5_Tables_Info_new_Jun2_smalllist.json"
adls_log_path = "abfss://gb-dl-v3-log@cdngbcacnpdevlogdl.dfs.core.windows.net/log/ingestion_Jun2"
audit_table_path = "abfss://gb-dl-v3-log@cdngbcacnpdevlogdl.dfs.core.windows.net/log/audit_table_ingestion_Jun2"  
reconciliation_table_path = "abfss://gb-dl-v3-log@cdngbcacnpdevlogdl.dfs.core.windows.net/log/recon_table_ingestion_Jun2"

# Read JSON file from ADLS using dbutils.fs
try:
    json_content = dbutils.fs.head(config_file_path)
    json_data = json.loads(json_content)
except Exception as e:
    logger.error(f"Error reading JSON file: {e}")
    exit(1)

# Extract project name
project_name = json_data.get("ProjectName", "V3")
logger.info(f"Project Name: {project_name}")

schema_names = []
table_names = []
module_names = []
s3_paths = []
adls_targets = []
checkpoint_locations = []

for schema in json_data['Schemas']:
    schema_name = schema['SchemaName']
    for table in schema['Tables']:
        table_name = table['TableName']
        module_name = table['ModuleName']
        schema_names.append(schema_name)
        table_names.append(table_name)
        module_names.append(module_name)
        s3_paths.append(f"{source_base_path}/{schema_name}/{table_name}")
        adls_targets.append(f"{target_base_path}/{table_name}")
        checkpoint_locations.append(f"{target_base_path}/{table_name}/checkpoint")

output_df = pd.DataFrame({
    "SchemaName": schema_names,
    "TableName": table_names,
    "ModuleName": module_names,
    "S3Path": s3_paths,
    "ADLSTarget": adls_targets,
    "CheckpointLocation": checkpoint_locations
})

# Define audit schema
audit_schema = StructType([
    StructField("etl_run_date", TimestampType(), True),
    StructField("table_name", StringType(), True),
    StructField("schema_name", StringType(), True),
    StructField("module_name", StringType(), True),
    StructField("total_records_source", DoubleType(), True),
    StructField("total_records_destination", DoubleType(), True),
    StructField("file_path", StringType(), True),
    StructField("file_name", StringType(), True),
    StructField("file_size_MB", DoubleType(), True),
    StructField("file_type", StringType(), True),
    StructField("ingestion_time", TimestampType(), True),
    StructField("load_time", TimestampType(), True),
    StructField("latency_seconds", DoubleType(), True),
    StructField("schema_validation", StringType(), True),
    StructField("hash_validation", StringType(), True)
])

# Create empty audit DataFrame
empty_audit_df = spark.createDataFrame([], audit_schema)
empty_audit_df.write.format("delta").mode("overwrite").save(audit_table_path)

# Define reconciliation schema
reconciliation_schema = StructType([
    StructField("etl_run_date", TimestampType(), True),
    StructField("table_name", StringType(), True),
    StructField("schema_name", StringType(), True),
    StructField("module_name", StringType(), True),
    StructField("source_type", StringType(), True),
    StructField("source_record_count", DoubleType(), True),
    StructField("target_record_count", DoubleType(), True),
    StructField("reconciliation_status", StringType(), True)
])

# Create empty reconciliation DataFrame
empty_reconciliation_df = spark.createDataFrame([], reconciliation_schema)
empty_reconciliation_df.write.format("delta").mode("overwrite").save(reconciliation_table_path)

def process_table(row):
    source_s3_path = row["S3Path"]
    target_table_path = row["ADLSTarget"]
    checkpoint_path = row["CheckpointLocation"]

    df_stream = (
        spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "parquet")
        .option("cloudFiles.includeExistingFiles", "true")
        .option("cloudFiles.schemaLocation", checkpoint_path)
        .load(source_s3_path)
        .withColumn("file_path", input_file_name())
        .withColumn("file_name", col("file_path").substr(lit(-1), lit(1)))
        .withColumn("file_size_MB", lit(0.0))
        .withColumn("file_type", lit("parquet"))
        .withColumn("ingestion_time", current_timestamp())
    )

    df_with_etl_run_dt = df_stream.withColumn("DL_ETL_RUN_DT", current_timestamp())

    query = (
        df_with_etl_run_dt.writeStream
        .option("checkpointLocation", checkpoint_path)
        .format("delta")
        .outputMode("append")
        .option("mergeSchema", "true")
        .start(target_table_path)
    )

    def update_audit_table(batch_df, batch_id):
        if batch_df.count() == 0:
            return

        table_name = row["TableName"]
        schema_name = row["SchemaName"]
        module_name = row["ModuleName"]
        source_type = "S3"

        source_df = spark.read.format("parquet").load(source_s3_path)
        total_records_source = float(source_df.count())

        if DeltaTable.isDeltaTable(spark, target_table_path):
            target_df = spark.read.format("delta").load(target_table_path)
        else:
            source_df.write.format("delta").mode("overwrite").save(target_table_path)
            target_df = spark.read.format("delta").load(target_table_path)
        
        total_records_destination = float(target_df.count())

        reconciliation_status = "Matched" if total_records_source == total_records_destination else "Mismatched"

        reconciliation_data = spark.createDataFrame(
            [(datetime.now(), table_name, schema_name, module_name, source_type, total_records_source, total_records_destination, reconciliation_status)],
            schema=reconciliation_schema
        )

        reconciliation_data.write.format("delta").mode("overwrite").save(reconciliation_table_path)

        file_path = batch_df.select("file_path").first()["file_path"]
        file_name = batch_df.select("file_name").first()["file_name"]
        file_size_MB = batch_df.select("file_size_MB").first()["file_size_MB"]
        file_type = batch_df.select("file_type").first()["file_type"]
        ingestion_time = batch_df.select("ingestion_time").first()["ingestion_time"]
        load_time = datetime.now()
        latency_seconds = (load_time - ingestion_time).total_seconds()

        audit_data = spark.createDataFrame(
            [(load_time, table_name, schema_name, module_name, total_records_source, total_records_destination, file_path, file_name, file_size_MB, file_type, ingestion_time, load_time, latency_seconds, "N/A", "N/A")],
            schema=audit_schema
        )

        delta_table = DeltaTable.forPath(spark, audit_table_path)
        retry_count = 0
        max_retries = 3
        while retry_count < max_retries:
            try:
                delta_table.alias("tgt").merge(
                    audit_data.alias("src"),
                    "tgt.table_name = src.table_name AND tgt.schema_name = src.schema_name AND tgt.module_name = src.module_name AND tgt.file_path = src.file_path"
                ).whenMatchedUpdate(
                    set={"tgt.total_records_destination": col("src.total_records_destination")}
                ).whenNotMatchedInsert(
                    values={
                        "etl_run_date": col("src.etl_run_date"),
                        "table_name": col("src.table_name"),
                        "schema_name": col("src.schema_name"),
                        "module_name": col("src.module_name"),
                        "total_records_source": col("src.total_records_source"),
                        "total_records_destination": col("src.total_records_destination"),
                        "file_path": col("src.file_path"),
                        "file_name": col("src.file_name"),
                        "file_size_MB": col("src.file_size_MB"),
                        "file_type": col("src.file_type"),
                        "ingestion_time": col("src.ingestion_time"),
                        "load_time": col("src.load_time"),
                        "latency_seconds": col("src.latency_seconds"),
                        "schema_validation": col("src.schema_validation"),
                        "hash_validation": col("src.hash_validation")
                    }
                ).execute()
                break
            except ConcurrentAppendException:
                retry_count += 1
                time.sleep(5)

        if total_records_source > 0:
            batch_df_final = batch_df.drop("file_path", "file_name", "file_size_MB", "file_type", "ingestion_time")
            batch_df_final.write.format("delta").mode("overwrite").save(target_table_path)
            logger.info(f"Overwritten data for table {table_name} with new data")

    df_with_etl_run_dt.writeStream.foreachBatch(update_audit_table).start()

    return query

def process_all_tables(output_df):
    num_tables = len(output_df)
    max_workers = num_tables
    queries = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_table, row) for index, row in output_df.iterrows()]
        for future in as_completed(futures):
            queries.append(future.result())
    return queries

queries = process_all_tables(output_df)
logger.info("Ingestion process completed for all tables.")